import sys, os
import random
import copy
import numpy as np

import torch
from arg_parser import init_args
from subexp_special_utils import RunExp, get_save_dir

UPPER_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__))) # NOTE: for .py
# UPPER_DIR = os.path.dirname(os.path.abspath(os.getcwd()))             # NOTE: for .ipynb
sys.path.append(UPPER_DIR)
from exp_special_utils import compute_pseudo_label_topoinf_wrapper

sys.path.append(os.path.dirname(UPPER_DIR))
from dataset_loader import DataLoader
from models import get_gnn_model
from topoinf_impl import TopoInf
from base_utils.base_general_utils import fix_seed, tab_printer
from base_utils.base_io_utils import analyse_and_save_recording
from base_utils.base_splitting_utils import print_pyg_data_split, rand_train_val_test_split_wrapper
from base_utils.base_pseudo_label_utils import get_pseudo_label_matrix


if __name__ == '__main__':
    ### Parse Args ###
    args = init_args(sys.argv[1:])
    args.save_dir = get_save_dir(args)      # NOTE: for saving experimental results
    tab_printer(args)

    ## Load data
    data = DataLoader(dataset_name = args.dataset,
                        # root_path = '../data/')
                        root_path = '../../data/')  # NOTE: specify correct data path
    if args.split_mode == 'ratio':  # 60%/20%/20%
        # NOTE: if args.split_mode == 'number', use 'public' splitting.
        fix_seed(args.seed)
        rand_train_val_test_split_wrapper(data, args)
    elif args.dataset in ['computers', 'photo']:
        # NOTE: there is no 'public' splitting in Amazon.
        fix_seed(args.seed)
        rand_train_val_test_split_wrapper(data, args)
    elif args.dataset in ['actor']:
        # NOTE: For Actor, train_mask.shape = [7600, 10].
        data.train_mask = data.train_mask[:, 0]
        data.test_mask = data.test_mask[:, 0]
        data.val_mask = data.val_mask[:, 0]
  
    print_pyg_data_split(data)
    print('DATA:', data)

    ## Define model ###
    Net = get_gnn_model(gnn_name = args.model)
    model = Net(data, args)
    print('MODEL:', model)

    ### Define loss function ###
    criterion = torch.nn.NLLLoss()
    print('CRITERION:', criterion)

    ### Transfer to device ###
    device = torch.device("cuda:"+str(args.device)) if torch.cuda.is_available() and args.device>=0 else torch.device("cpu")
    data = data.to(device)
    model = model.to(device)

    ### Generate SEEDS for all runs ###
    fix_seed(args.seed)
    MAX_INT_32 = 2**32 - 1
    SEEDS = [random.randint(0, MAX_INT_32) for _ in range(args.n_runs)]  # NOTE: numpy seed only accept [0, 2**32 - 1]
    print(f"All Run Seeds Generated by Global Seed [{args.seed}]: {SEEDS}")
    
    
    recording_dict = copy.deepcopy(vars(args))  # save experimental results.

    for run_index in range(args.n_runs):
        record_dict_per_run = {}
        
        seed = SEEDS[run_index]
        record_dict_per_run['seed'] = seed
        
        ### wo_dropedge ###
        best_model, best_eval_result = RunExp(data, model, args, criterion, 
                                run_index=run_index, seed=seed,
                                save_file_suffix="wo_dropedge",
                                dropedge=False, topoinf_all_e=None,
                                return_model=True)
        record_dict_per_run['wo_dropedge'] = best_eval_result

        ### Get Pseudo Label Matrix ###
        pseudo_label_matrix = get_pseudo_label_matrix(best_model, data, args)
        
        ## Check Correctness ###
        mean_square = (pseudo_label_matrix[data.test_mask] ** 2).sum(dim=1).sqrt().mean()
        print(f'[Pseudo Label Info] Mean Square: {mean_square:.2f}')
        sample_k = 5
        sample_indices = [random.randint(0, args.num_test-1) for _ in range(sample_k)]
        print('[Pseudo Label Info] Sampled Prob Vectors: \n', pseudo_label_matrix[data.test_mask][sample_indices])
        
        
        #calculate coefficients
        coefficients = None
        if args.model not in ['GPRGNN' ,'BERNNET']:
            if args.coefficients == 'last':
                coefficients = [0.0] * args.k_order
                coefficients[-1] = 1.0
            else:
                coefficients = [1.0/args.k_order] * args.k_order
        print(f'COEFFICIENTS: {coefficients}')
       
        if args.model in ['GPRGNN', 'BERNNET']:
            if args.model == 'GPRGNN':
                coefficients = best_model.get_theta()
                print(f'[Orginal Coefficients of GPRGNN on {args.dataset}]: {coefficients}')
                coefficients = coefficients[1:]
                coefficients = (coefficients / np.sum(coefficients)).tolist()
            elif args.model == 'BERNNET':
                theta = best_model.get_theta()
                print(f'[Orginal Theta of BERNNET on {args.dataset}]: {theta}')
                a3 = theta[0] + 3 * theta[2] - 3 * theta[1] - theta[3]
                a2 = theta[3] * 3 - 3 * theta[2] - 3 * theta[1]
                a1 = 3 * theta[1] -3 * theta[3] - 3 * theta[2]
                a0 = theta[3] + 3 * theta[1] + 3 * theta[2]
                print(f'[Orginal Coefficients of BERNNET on {args.dataset}]: {[a0, a1, a2, a3]}')
                s = sum([a1, a2, a3])
                coefficients = [a1 / s, a2 / s, a3 / s]
                print(f'[Normalized Coefficients of BERNNET on {args.dataset}]: {coefficients}')

            
        topoinf_calculator = TopoInf(data = data, 
                    lambda_reg = args.lambda_reg,   
                    with_self_loops = not args.without_self_loops,
                    k_order = args.k_order,
                    coefficients = coefficients,
                    distance_metric_name = args.distance_metric
                    )    
        topoinf_all_e = compute_pseudo_label_topoinf_wrapper(topoinf_calculator, pseudo_label_matrix=pseudo_label_matrix.cpu(), 
                                                             data=data, args=args)
        
        ### w_general_dropedge ###
        best_eval_result = RunExp(data, model, args, criterion, 
                                run_index=run_index, seed=seed,
                                save_file_suffix="w_general_dropedge",
                                dropedge=True, topoinf_all_e=None)
        record_dict_per_run['w_general_dropedge'] = best_eval_result

        ### w_pos_topoinf_dropedge ###
        best_eval_result  = RunExp(data, model, args, criterion, 
                                run_index=run_index, seed=seed,
                                save_file_suffix="w_pseudo_label_guided_dropedge",
                                dropedge=True, topoinf_all_e=topoinf_all_e ,coefficients = coefficients)
        record_dict_per_run['w_pseudo_label_guided_dropedge'] = best_eval_result

        recording_dict[f'run_[{run_index+1}]'] = record_dict_per_run

    
    analyse_and_save_recording(recording = recording_dict, 
                               analysis_attr_list = ['wo_dropedge', 'w_general_dropedge', 'w_pseudo_label_guided_dropedge'], 
                               save_dir = args.save_dir,
                               args = args)
