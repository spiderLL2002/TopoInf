import sys, os
import random
import copy
import numpy as np

import torch
from arg_parser import init_args

UPPER_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__))) # NOTE: for .py
# UPPER_DIR = os.path.dirname(os.path.abspath(os.getcwd()))             # NOTE: for .ipynb
sys.path.append(UPPER_DIR)
from exp_special_utils import RunExp, topoinf_based_deleting_edges
from subexp_special_utils import get_save_dir, analyse_and_save_recording

sys.path.append(os.path.dirname(UPPER_DIR))
from dataset_loader import DataLoader
from models import get_gnn_model
from topoinf_impl import TopoInf
from base_utils.base_general_utils import fix_seed, tab_printer

from base_utils.base_splitting_utils import print_pyg_data_split, rand_train_val_test_split_wrapper
from base_utils.base_pseudo_label_utils import get_pseudo_label_matrix, compute_pseudo_label_topoinf
from base_utils.base_training_utils import eval, print_eval_result
from base_utils.model_2_filter import model_2_filter

if __name__ == '__main__':

    ### Parse Args ###
    args = init_args(sys.argv[1:])
    args.save_dir = get_save_dir(args)      # NOTE: for saving experimental results
    tab_printer(args)


    ## Load data
    data = DataLoader(dataset_name = args.dataset,
                        root_path = '../data/')  # NOTE: specify correct data path
    if args.split_mode == 'ratio':  # 60%/20%/20%
        # NOTE: if args.split_mode == 'number', use 'public' splitting.
        fix_seed(args.seed)
        rand_train_val_test_split_wrapper(data, args)
    elif args.dataset in ['computers', 'photo']:
        # NOTE: there is no 'public' splitting in Amazon.
        fix_seed(args.seed)
        rand_train_val_test_split_wrapper(data, args)
    elif args.dataset in ['actor', 'texas', 'wisconsin', 'amazon-ratings', 'roman-empire']:
        # NOTE: For Actor, train_mask.shape = [7600, 10].
        data.train_mask = data.train_mask[:, 0]
        data.test_mask = data.test_mask[:, 0]
        data.val_mask = data.val_mask[:, 0]
    print_pyg_data_split(data)
    print('DATA:', data)

    ## Define model
    Net = get_gnn_model(gnn_name = args.model)
    model = Net(data, args)
    print('MODEL:', model)

    ### Define loss function ###
    criterion = torch.nn.NLLLoss()
    print('CRITERION:', criterion)

    ### Transfer to device ###
    device = torch.device("cuda:"+str(args.device)) if torch.cuda.is_available() and args.device>=0 else torch.device("cpu")
    data = data.to(device)
    model = model.to(device)

    ### Generate SEEDS for all runs ###
    fix_seed(args.seed)
    MAX_INT_32 = 2**32 - 1
    SEEDS = [random.randint(0, MAX_INT_32) for _ in range(args.n_runs)]  # NOTE: numpy seed only accept [0, 2**32 - 1]
    print(f"All Run Seeds Generated by Global Seed [{args.seed}]: {SEEDS}")
    
    recording_dict = copy.deepcopy(vars(args))
    #calculate coefficients
    coefficients = None
    if args.model not in ['GPRGNN' ,'BERNNET']:
        if args.coefficients == 'last':
            coefficients = [0.0] * args.k_order
            coefficients[-1] = 1.0
        else:
            coefficients = [1.0/args.k_order] * args.k_order
        print(f'COEFFICIENTS: {coefficients}')
    
    for run_index in range(args.n_runs):
        record_dict_per_run = {}
        
        seed = SEEDS[run_index]
        record_dict_per_run['seed'] = seed
        ### Run Model ###
        best_model, best_eval_result_before_topoinf = \
            RunExp(data, model, args, criterion, 
                    run_index=run_index, seed=seed,
                    save_file_suffix='before_topoinf',
                    return_model=True)
        record_dict_per_run['before_topoinf'] = best_eval_result_before_topoinf
        
        if args.model in ['GPRGNN', 'BERNNET']:
            if args.model == 'GPRGNN':
                coefficients = best_model.get_theta()
                print(f'[Orginal Coefficients of GPRGNN on {args.dataset}]: {coefficients}')
                coefficients = coefficients[1:]
                coefficients = (coefficients / np.sum(coefficients)).tolist()
            elif args.model == 'BERNNET':
                theta = best_model.get_theta()
                print(f'[Orginal Theta of BERNNET on {args.dataset}]: {theta}')
                a3 = theta[0] + 3 * theta[2] - 3 * theta[1] - theta[3]
                a2 = theta[3] * 3 - 3 * theta[2] - 3 * theta[1]
                a1 = 3 * theta[1] -3 * theta[3] - 3 * theta[2]
                a0 = theta[3] + 3 * theta[1] + 3 * theta[2]
                print(f'[Orginal Coefficients of BERNNET on {args.dataset}]: {[a0, a1, a2, a3]}')
                s = sum([a1, a2, a3])
                coefficients = [a1 / s, a2 / s, a3 / s]
                print(f'[Normalized Coefficients of BERNNET on {args.dataset}]: {coefficients}')
 
        topoinf_calculator = TopoInf(data = data, 
                lambda_reg = args.lambda_reg,   
                with_self_loops = not args.without_self_loops,
                k_order = args.k_order,
                coefficients = coefficients,
                distance_metric_name = args.distance_metric.replace('_', ' ')
                )

        
        ### Get Pseudo Label Matrix ###
        pseudo_label_matrix = get_pseudo_label_matrix(best_model, data, args)
        
        ## Check Correctness ##
        mean_square = (pseudo_label_matrix[data.test_mask] ** 2).sum(dim=1).sqrt().mean()
        print(f'[Pseudo Label Info] Mean Square: {mean_square:.2f}')
        sample_k = 5
        num_test = data.test_mask.sum().item()
        sample_indices = [random.randint(0, num_test-1) for _ in range(sample_k)]
        print('[Pseudo Label Info] Sampled Prob Vectors: \n', pseudo_label_matrix[data.test_mask][sample_indices])
        
        ### Calculate and Save TopoInf ###
        topoinf_all_e = compute_pseudo_label_topoinf(topoinf_calculator, pseudo_label_matrix=pseudo_label_matrix.cpu(), args=args)

        unit_value_list = args.delete_num_list if args.delete_unit in ['number'] else args.delete_rate_list
        for unit_value in unit_value_list:
            key = f'delete_[{args.delete_unit}]_[{unit_value}]'
            record_dict_per_run[key] = {}
            
            ### Delete Edges ###
            data_topoinf = copy.deepcopy(data)
            data_topoinf.edge_index = topoinf_based_deleting_edges(data, topoinf_all_e, unit_value, args)

            ### Eval `best_model` on Topology with TopoInf deleting
            eval_result_after_topoinf_before_retrain = eval(best_model, data_topoinf, criterion=None, get_detail=False)
            print_eval_result(eval_result_after_topoinf_before_retrain, prefix=f'[After TopoInf ({key}) but NOT Retrain]')
            record_dict_per_run[key]['before_retrain'] = eval_result_after_topoinf_before_retrain

            ### Rerun Model ###
            best_eval_result_after_topoinf_after_retrain = \
                RunExp(data_topoinf, model, args, criterion, 
                    run_index=run_index, seed=seed,
                    save_file_suffix=f'after_{key}',
                    return_model=False)
            record_dict_per_run[key]['after_retrain'] = best_eval_result_after_topoinf_after_retrain

        recording_dict[f'run_[{run_index+1}]'] = record_dict_per_run


    analyse_and_save_recording(recording = recording_dict, 
                               args = args)
