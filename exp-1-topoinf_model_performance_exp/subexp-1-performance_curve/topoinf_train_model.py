import sys, os
import random
import copy
import torch
 
from subexp_special_utils import get_save_dir
from arg_parser import init_args
# UPPER_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))         # NOTE: for .py
UPPER_DIR = os.path.dirname(os.path.abspath(os.getcwd()))    # NOTE: for .ipynb
sys.path.append(UPPER_DIR)
from exp_special_utils import RunExpWrapper, get_topoinf_wrapper, topoinf_based_deleting_edges,get_edges_nums

sys.path.append(os.path.dirname(UPPER_DIR))
from dataset_loader import DataLoader
from models import get_gnn_model
from base_utils.base_general_utils import fix_seed, tab_printer
from base_utils.base_io_utils import save_recording, make_multi_level_dir
from base_utils.base_splitting_utils import print_pyg_data_split, rand_train_val_test_split_wrapper


if __name__ == '__main__':
    
    ### Parse Args ###
    args = init_args(sys.argv[1:])
    tab_printer(args)

    ## Load data
    data = DataLoader(dataset_name = args.dataset,
                        root_path = '../../data/')  # NOTE: specify correct data path
    
    if args.split_mode == 'ratio':  # 60%/20%/20%
        # NOTE: if args.split_mode == 'number', use 'public' splitting.
        fix_seed(args.seed)
        rand_train_val_test_split_wrapper(data, args)
    elif args.dataset in ['computers', 'photo']:
        # NOTE: there is no 'public' splitting in Amazon.
        fix_seed(args.seed)
        rand_train_val_test_split_wrapper(data, args)
    elif args.dataset in ['actor']:
        # NOTE: For Actor, train_mask.shape = [7600, 10].
        data.train_mask = data.train_mask[:, 0]
        data.test_mask = data.test_mask[:, 0]
        data.val_mask = data.val_mask[:, 0]
    
    print_pyg_data_split(data)
    #print('DATA:', data)

    ### Define loss function ###
    criterion = torch.nn.NLLLoss()
    print('CRITERION:', criterion)

    ### Transfer to device ###
    device = torch.device("cuda:"+str(args.device)) if torch.cuda.is_available() and args.device>=0 else torch.device("cpu")
    data = data.to(device)
    

    ### Generate SEEDS for all runs ###
    fix_seed(args.seed)
    MAX_INT_32 = 2**32 - 1
    SEEDS = [random.randint(0, MAX_INT_32) for _ in range(args.n_runs)]  # NOTE: numpy seed only accept [0, 2**32 - 1]
    print(f"All Run Seeds Generated by Global Seed [{args.seed}]: {SEEDS}")

    # recording_dict = vars(args)   # NOTE: `vars(args)` share space with `args`!
    recording_dict = copy.deepcopy(vars(args))

    topoinf_all_e = get_topoinf_wrapper(data, args)
    #args.model_list, choices=['GCN', 'SGC', 'APPNP', 'MLP', 'GPRGNN', 'BERNNET'], 
    for model_name in args.model_list:
        ## Define model
        args.model = model_name
        Net = get_gnn_model(gnn_name = args.model)
        model = Net(data, args)
        print('MODEL:', model)
        model = model.to(device)

        if model_name not in recording_dict:
            recording_dict[model_name] = {}

        # NOTE: no delete
        args.skip_delete = True
        analysed_result = RunExpWrapper(data, model, args, criterion, SEEDS)
        recording_dict[model_name]['none'] = analysed_result
        if model_name not in ['MLP']:
            args.skip_delete = False
            pos_num , neg_num = get_edges_nums(topoinf_all_e, args)   
            #第二层循环 ,args.delete_mode_list,  choices=['pos', 'neg'],
            for delete_mode in args.delete_mode_list:
                data_delete_iteration = copy.deepcopy(data)
                topoinf_all_e_delete_iteration = copy.deepcopy(topoinf_all_e)
                edges_haven_deleted = set()
                args.delete_mode = delete_mode
                if delete_mode not in recording_dict[model_name]:
                    recording_dict[model_name][delete_mode] = {}
                #删除的比例 default=[0.1]*9, 
                delete_mag_list = args.delete_rate_list if args.delete_unit in ['mode_ratio', 'ratio'] \
                                else args.delete_num_list
                # magnitude -强度
                for delete_mag in delete_mag_list:
                    if len(edges_haven_deleted) > pos_num +neg_num :
                        break
                    if args.delete_unit in ['mode_ratio', 'ratio']:
                        args.delete_rate = delete_mag
                        args.delete_num = pos_num*delete_mag if delete_mode == "pos" else neg_num *delete_mag
                    else:
                        args.delete_num = delete_mag
                    args.delete_num = int(args.delete_num)
                    if args.save_detailed_perf or args.save_reduced_perf:
                        args.save_dir = get_save_dir(args)      # NOTE: for saving experimental results
                    ### Delete edges based on TopoInf ###    
                    fix_seed(args.seed)
                    topoinf_all_e_delete_iteration ,data_delete_iteration = topoinf_based_deleting_edges(edges_haven_deleted , data_delete_iteration,topoinf_all_e_delete_iteration, args )
                    analysed_result = RunExpWrapper( data_delete_iteration, model, args, criterion, SEEDS,pos_num ,neg_num , edges_haven_deleted )
                    #analysed_result['delete_info'] = delete_info
                    recording_dict[model_name][delete_mode][delete_mag] = analysed_result
        
        # save recording when finished one model
        save_recording(recording = recording_dict[model_name], 
                    save_dir = make_multi_level_dir([args.perf_save_root_dir, args.dataset.lower(), args.delete_strategy]),
                    recording_file = f'{model_name}_analysed_recording.json')

    save_recording(recording = recording_dict, 
                    save_dir = make_multi_level_dir([args.perf_save_root_dir, args.dataset.lower(), args.delete_strategy]),
                    recording_file = f'analysed_recording.json')